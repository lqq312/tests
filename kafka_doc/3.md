# kafka架构

![](/Users/luqq/Documents/02_tec-doc/kafka_doc/pic/3.1.png)

kafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。

topic是逻辑上的概念，而partition是物理上的概念，每个partition对应一个数据目录，该数据目录中存储的就是producer生产的数据。Producer和产的数据会被不断追加到该数据文件的末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset上，以便出错恢复时，从上次的位置继续消费。

每个分区目录下的.log文件用于存放消息数据，而.index文件用于记录消费者消费到的offset；

![](/Users/luqq/Documents/02_tec-doc/kafka_doc/pic/3.2.png)

由于生产者生产消息会不断的追加到.log文件的末尾，为防止.log文件过大导致数据定位效率低下，kafka采用了分片和索引的机制，将每个partition分为多个segment。每个segment对应两个文件——“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号。

index和log文件以当前segment的第一条消息的offset命名。

![](/Users/luqq/Documents/02_tec-doc/kafka_doc/pic/3.3.png)

## kafka分区策略

1. 分区的原因，**便于在集群中扩展**，每个partition可以通过调整以适应它所在的机器，而一个topic又可以有多个partition组成，因此整个集群就可以适应任意大小的数据了；**可以提高并发**，因为可以以partition为单位进行读写了。
2. 分区的原则，我们需要将producer发送的数据封装成一个producerRecord对象。

    * 指明partition的情况，直接将指明的值作为partition值；
    * 没有指明的partition值但有key的情况下，将key的hash值与topic的partition数进行取模得到partition值。
    * 既没有partition值又没有key值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与topic可用的partition总数取余得到partiton值，也就是常说的round-robin算法。

## 数据可靠性保证

为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新改善数据。

![](/Users/luqq/Documents/02_tec-doc/kafka_doc/pic/3.4.png)

副本数据同步策略：

方案 | 优点 | 缺点
-|-|-
半数以上完成同步，就发送ACK | 延迟低 | 选举新的leader时，容忍n台节点的故障，需要2n+1个副本
全部完成同步，才发送ACK | 选举新的leader时，容忍n台节点的故障，需要n+1个副本 | 延迟高

kafka使用的是第二种方案：

* 同样为了容忍n台节点故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。
* 虽然第二种方案的网络延迟会比较高，但网络延迟对kafka的影响小。

ISR（in-sync replica set）：采用第二种方案之后，设想以下情景，leader收到数据，所有follower都开始同步数据，但有一个follwer，因为某种故障，迟迟不能与leader进行同步，因此kafka采用了ISR的方案。leader维护了一个动态的in-sync replica set（ISR），意为和leader保持同步的follower集合。当ISR中的follower完成数据同步之后，leader就会给follower发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由“replica.lag.time.max.ms”参数定义。Leader发生故障之后，就会从ISR中选举出新的Leader。

ACK应答机制：

对于某些不太重要的数据，对数据的可靠性要求不高，能够容忍少量的数据丢失，所以没必要等ISR中的follower全部接收成功。
所以kafka为用户提供了三种可靠性级别，用户可根据可靠性和延迟的要求进行权衡，可选择如下的配置。

acks的参数配置：

    0：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据。
    1：producer等待broker的ack，partition的leader落盘成功后返回ack，如果的follower同步成功之前leader故障，那么将会丢失数据。
    -1（all）：producer等待broker的ack，partition的leader和follower（ISR中的follower）全部落盘成功后才返回ack，但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成数据重复。

## 数据一致性

LEO：Log End Offset，每个副本的最后一个offset即最大的offset；
HW：High Watermark，所有副本中最小的LEO，只有HW之前的数据才对Consumer可见，即消费者能见到的最大offset；

![](/Users/luqq/Documents/02_tec-doc/kafka_doc/pic/3.5.png)

* follower故障，follower故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分裁剪掉，从HW开始向leader进行同步，等该follower的LEO大于等于该partition的HW，即follower追上leader之后，就可以重新加入ISR了。
* leader故障，leader故障发生之后，会从ISR中选出一个新的leader，然后为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分裁剪，然后从新的leader同步数据。

***注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。***

Exactly Once：

将服务器的ACK级别设置为-1，可以保证Producer到Server之间不会丢失数据，即At Least Once。相对的，将服务器ACK级别设置为0，可以保证生产者每条消息只会被发送一次，即At Most Once。

At Least Once可以保证数据不丢失，但是不能保证数据不重复；相对的，At Least Once可以保证数据不重复，但是不能保证数据不丢失。但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即Exactly Once。在0.11版本以前的kafka是无法保障的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大的影响。

0.11版本的kafka，引入了一项重大特性：幂等性。即Producer不论向Server发送多少次重复数据，Server端都只会持久化一条。幂等性结合At Least Once，就构成了kafka的Exactly Once。

要启用幂等性，只需要将Producer的参数中enable.idompotence设置为true即可，kafka的幂等性实现其实是将原来下游需要做的去重放在了数据的上游。开启幂等性的Producer在初始化的时候会被分配一个PID（Producer ID），发往同一partition的消息会附带Sequence Number。而Broker端会对<PID,Partition,SqeNumber>做缓存，当具有相同主键的消息提交时，Broker只会持久化一条。

但是PID重启就会变化，同时不同的partition也具有不同主键，所以幂等性无法保证跨分区跨会话的Exactly Once。

## 消费者

### 消费方式

**consumer采用pull（拉）模式从broker中读取数据。**

**push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。**它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。

**pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。**针对这一点，kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout。

### 分区分配策略

一个consumer group中有多个consumer，一个topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。

kafka有两种分配策略，一个是RoundRobin，一个是Range。

![](/Users/luqq/Documents/02_tec-doc/kafka_doc/pic/3.6.jpeg)

![](/Users/luqq/Documents/02_tec-doc/kafka_doc/pic/3.7.jpg)

默认使用Range，使用Range会导致一个消费者组在消费多个不同topic时组内的多个消费者消费不平均。

如consumer group内的consumer的数量发生变化时会重新分配分区。

### offset维护

由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。

kafka 0.9版本之前，consumer默认将offset保存在zookeeper中，从0.9版本开始，consumer默认将offset保存在kafka一个内置的topic中，该topic为__consumer_offset。

如需读取系统自建的topic需按如下步骤进行操作：

1. 修改consumer.properties配置文件

    `exclude.internal.topics=false`

2. 读取offset

0.11.0.0之前的版本：

    `bin/kafka-console-consumer.sh --tipic __consumer_offset --zookeeper kafka1:2181 --formatter "kafka.coordinator.GroupMetadataManager\$OffsetsMessageFormatter" --consumer.config config/consumer.porperties --from-beginning`

0.11.0.0之后的版本（含）

    `bin/kafka-console-consumer.sh --tipic __consumer_offset --zookeeper kafka1:2181 --formatter "kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter" --consumer.config config/consumer.porperties --from-beginning`

## kafka高效读写数据

1. 顺序写磁盘

kafka的producer生产数据，要定入到log文件中，定的过程是一直追加到文件的末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到600M/s，而随机写只有100K/s。这与磁盘的机械结构有关，顺序写之所以快，是因为其省去了大量磁头寻址时间。

2. 零复制技术

![](/Users/luqq/Documents/02_tec-doc/kafka_doc/pic/3.8.png)

3. kafka集群使用了分区的技术，从而实现并发读写










